# Realâ€‘Time Poseâ€‘Based Exercise Recognition with Biâ€‘LSTM

> **ECE 5831 â€” Pattern Recognition & Neural Networks**  
> **Student:** Hemanth Katikala Muniraj  
> **Classes:** `bench`, `sit_ups`, `squat`  
> **Goal:** Realâ€‘time recognition of exercise type from webcam/video using MediaPipe Pose + poseâ€‘feature + a 2â€‘layer Biâ€‘LSTM. Includes a **squatâ€‘only rep counter** based on kneeâ€‘angle hysteresis.

---

## ğŸ“º Deliverables

- **Demo video (YouTube):** `TODO: https://youtu.be/...`
- **Slide deck (Google Drive):** `TODO: https://drive.google.com/...`
- **Report (IEEE PDF):** `TODO: https://drive.google.com/...`
- **Dataset (Drive or instructions):** `TODO: https://drive.google.com/...`
- **Interactive notebook:** [`final-project.ipynb`](./final-project.ipynb)
- **Annotated demo output:** `annotated_output.mp4` (created by the notebook or inference script)

---

## ğŸ§­ Repository Structure

```
.
â”œâ”€ final-project.ipynb                 # Minimal demo: loads best.pt, runs inference, saves annotated MP4
â”œâ”€ train_sequence_nn.py                # Train 2Ã—Biâ€‘LSTM on windowed pose features â†’ best.pt, scaler, labels, results
â”œâ”€ infer_on_video_bilstm.py            # Robust video inference + squatâ€‘only reps + bench plausibility checks
â”œâ”€ extract_ranges_to_csv_from_frames.py# Build ranges_dataset.csv from frame folders (bench/sit_ups/squat)
â”œâ”€ csv_grouped.py           # (Optional) Build ranges_dataset_grouped.csv from Penn Action .mat labels
â”œâ”€ runs/
â”‚   â””â”€ train_bilstm/
â”‚       â”œâ”€ best.pt                     # Trained checkpoint (with config + labels)
â”‚       â”œâ”€ scaler.joblib               # sklearn StandardScaler used at train time
â”‚       â”œâ”€ labels.json                 # ["bench","sit_ups","squat"]
â”‚       â”œâ”€ results.csv                 # Perâ€‘epoch metrics
â”‚       â”œâ”€ results.png                 # YOLOâ€‘style plots (loss/acc + best epoch marker)
â”‚       â”œâ”€ confmat_val.png             # Validation confusion matrix
â”‚       â””â”€ confmat_test.png            # Test confusion matrix (if generated)
â””â”€ README.md
```

---

## ğŸ§© System Overview

1. **Pose extraction (MediaPipe Pose)** â†’ 33 landmarks per frame.  
2. **Feature engineering** â†’ 8 joint angles (knees/hips/elbows/shoulders) + 2 normalized distances (`hip_y`, `wrist_y` normalized by shoulder width).  
3. **Temporal modeling** â†’ 60â€‘frame sliding windows (~2 s at 30 FPS) into a **2â€‘layer Biâ€‘LSTM (128 units/layer) + Dense(256)+Dropout**.  
4. **Inference** â†’ Live/video: pose â†’ features â†’ sliding window â†’ predicted class (`bench | sit_ups | squat`).  
5. **Rep counting** â†’ **Squats only**, using kneeâ€‘angle thresholds (down < ~95Â°, up > ~160Â°) with hysteresis.  
6. **Bench robustness** â†’ Upscaling, ROI retry, EMA landmark smoothing, shortâ€‘gap feature reuse, and **bench plausibility checks** reduce â€œunknown/flickerâ€ when wrists are confused with ankles.

---

## ğŸ“¦ Requirements

- Python **3.10â€“3.11** recommended
- Packages:
  - `torch` (CPU or CUDA)
  - `mediapipe`
  - `opencv-python`
  - `numpy`, `pandas`
  - `scikit-learn` (for `StandardScaler` deserialization)
  - `joblib`
  - `matplotlib`

**Quick install (pip):**
```bash
python -m venv .venv
# Windows:
. .venv\\Scripts\\activate
# macOS/Linux:
# source .venv/bin/activate

pip install --upgrade pip
pip install torch mediapipe opencv-python numpy pandas scikit-learn==1.3.2 joblib matplotlib
```

---

## ğŸ“‚ Data Preparation

Two supported layouts.

### A) Frame folders by class (recommended for quick start)
```
D:/Dataset/
 â”œâ”€ bench/
 â”‚   â”œâ”€ 0341/    # frames: 00001.jpg, 00002.jpg, ...
 â”‚   â”œâ”€ 0342/ 
 â”‚   â””â”€ 0480/
 â”œâ”€ sit_ups/
 â”‚   â”œâ”€ 1559/ ... 1658/
 â””â”€ squat/
     â”œâ”€ 1659/ ... 1889/
```

Generate `ranges_dataset.csv` from frames:
```bash
python extract_ranges_to_csv_from_frames.py --root "D:/Dataset" --out "ranges_dataset.csv"
```

Columns include: `lknee, rknee, lhip, rhip, lelbow, relbow, lshoulder, rshoulder, hip_y, wrist_y, label, group_id, frame_idx`.

### B) From Penn Action label .mat files
If you have `Penn_Action/labels/*.mat`, build a leakageâ€‘safe index:
```bash
python csv_grouped.py   # writes penn_labels_index.csv and prints targets
# then run your extractor to produce ranges_dataset_grouped.csv
```

> **Important:** Split **by `group_id` (session)** *before* windowing to avoid train/val/test leakage.

---

## ğŸ‹ï¸ Training the Biâ€‘LSTM

```bash
python train_sequence_nn.py --csv_path ranges_dataset.csv --epochs 50 --batch_size 64
```

**Default hyperparameters** (also saved into `best.pt â†’ config`):
- `WINDOW_SIZE=60`, `HOP_TRAIN=30`, `HOP_VAL=30`
- BiLSTM: `layers=2`, `hidden=128`, `bidirectional=True`
- Dense head `256` + `Dropout(0.5)`
- `AdamW`, `weight_decay=1e-4`, `label_smoothing=0.05`, gradient clip `5.0`
- `ReduceLROnPlateau` on val accuracy, early stop on patience

**Outputs** in `runs/train_bilstm/`:
- `best.pt`, `scaler.joblib`, `labels.json`
- `results.csv`, `results.png`, `confmat_*.png`
- Console prints: **validation accuracy**, **perâ€‘class P/R/F1**, **confusion matrix**

---

## ğŸ¬ Inference on a Video (robust, with squat reps)

Edit paths inside `infer_on_video_bilstm.py`, then run:
```bash
python infer_on_video_bilstm.py
```
- Overlays: `pred: <label> | Reps: <n>`  
- Writes `annotated_output.mp4`  
- Benchâ€‘stability features:
  - Upscale for pose if frame < `720px` tall (`MIN_INFER_H`)
  - EMA landmark smoothing (`EMA_ALPHA`)
  - Retry pose on torso ROI (`ROI_GROW`)
  - Shortâ€‘gap feature reuse (`MISS_TOL`)
  - **Bench plausibility checks** to reject impossible wrist/shoulder geometry

---

## ğŸ§ª Minimal Demo Notebook

Open **[`final-project.ipynb`](./final-project.ipynb)** and edit:
```python
MODEL_PATH   = r"C:/Users/DELL/Documents/ece5831-2025-assignments/ece5831-2025-final-project/runs/train_bilstm/best.pt"
SCALER_PATH  = r"C:/Users/DELL/Documents/ece5831-2025-assignments/ece5831-2025-final-project/runs/train_bilstm/scaler.joblib"
LABELS_PATH  = r"C:/Users/DELL/Documents/ece5831-2025-assignments/ece5831-2025-final-project/runs/train_bilstm/labels.json"
INPUT_VIDEO  = r"C:/Users/DELL/Documents/ece5831-2025-assignments/ece5831-2025-final-project/Videos/Squat.mp4"

```
Run all cells to:
- Load the trained model + scaler + labels
- Run MediaPipe Pose + Biâ€‘LSTM on a short clip
- Save **`annotated_output.mp4`**
- Show a few sampled frames inline

If you see `ModuleNotFoundError: sklearn`, run in a new cell:
```python
import sys, subprocess
subprocess.check_call([sys.executable, "-m", "pip", "install", "scikit-learn==1.3.2", "joblib"])
```

---

## ğŸ“Š Current Validation Results

- **Val Acc:** `0.9812` (best epoch: 36)

Perâ€‘class P/R/F1 (support):
- **bench:** 0.980 / 0.943 / 0.962 (n=159)  
- **sit_ups:** 0.972 / 0.993 / 0.983 (n=142)  
- **squat:** 0.985 / 0.994 / 0.990 (n=337)

*Splits are by `group_id` (session), not by frame.*

---

## ğŸ©¹ Troubleshooting

- **`ModuleNotFoundError: sklearn`** when loading `scaler.joblib`  
  Install `scikit-learn==1.3.2` and reâ€‘run.

- **`Cannot open video`**  
  Check `INPUT_VIDEO` path and codec; try `mp4v` writer; ensure file is accessible.

- **Pose misses wrists/arms on bench**  
  Increase `MIN_INFER_H` to `900`, set `POSE_DET_CONF=0.8`, `POSE_TRK_CONF=0.8`, increase `PRED_SMOOTH_N` to `9â€“11`.  
  Prefer higherâ€‘contrast clothing and a perpendicular camera angle.

- **Validation too high**  
  Ensure `ranges_dataset*.csv` is split **by `group_id` before windowing**.

---

## ğŸ”— Submission Links (fill these in)

- **YouTube demo:** `TODO`
- **Slides (Drive):** `TODO`
- **Report (IEEE PDF, Drive):** `TODO`
- **Dataset (Drive):** `TODO`

---

## âœ… Submission Checklist

- [ ] Repo name is **`ece-5831-2025-final-project`** and is **Public**  
- [ ] `final-project.ipynb` runs endâ€‘toâ€‘end and shows **executed cells**  
- [ ] `README.md` contains links to **demo video**, **slides**, and **report**  
- [ ] **Google Drive** folder `ece-5831-2025-final-project` with `dataset/`, `presentation/`, `report` is shared â€œAnyone with the linkâ€  
- [ ] **YouTube** demo uploaded and linked  
- [ ] `runs/train_bilstm/` includes `best.pt`, `scaler.joblib`, `labels.json`, `results.csv`, plots, `confmat_*.png`

---

## ğŸ“š References / Acknowledgments

- MediaPipe Pose â€” Google Research  
- Penn Action Dataset â€” Zhang et al., UPenn  
- PyTorch, scikitâ€‘learn, OpenCV, NumPy, Matplotlib  

*(Full citations will be included in the IEEE report.)*

---

## ğŸ”’ License & Academic Use

This repository is for educational purposes for **ECE 5831**. If redistributing dataset frames, ensure compliance with the original dataset license/terms. For Penn Action, link to the source or provide your own captured data.
